{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Tutorial 1 - Exploring the data.ipynb","provenance":[{"file_id":"131wXkv-ctD4qQUL6ecQCZDmQ3HivisB5","timestamp":1579008285110}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"Liumf1BGmJC4","colab_type":"text"},"source":["## **Discovery Taxonomy prediction from descriptions**\n","For this notebook we will use data from Discovery to further explore some of the ideas from the previous session. In the advanced search of Discovery, it is possible to filter your search according to a taxonomy which is maintained by the cataloguing team. There are 136 categories in the dataset with names such as \"Trade and commerce\", \"Population\", \"International\", \"Hunting\", and \"UFOs\". Each record in the catalogue is categorised according to a set of rules which are applied to the description of the record in Discovery. For example, if a description includes the following words or phrases it will be classified as \"Taxation\":  \"taxes\", \"taxation\", \"first fruits and tenths\", \"Domesday\", \"customs revenue\", \"scutages\".\n","\n","Some of these rules can be long and complex, and it is easy to find examples where the ambiguities of language mean they categorise documents incorrectly. We're not going to solve these problems in this tutorial but it is an opportunity to explore a machine learning approach to the problem and discuss the advantages and disadvantages of Rules vs. ML."]},{"cell_type":"code","metadata":{"id":"3E8Vh0sOBXDl","colab_type":"code","outputId":"ea495c00-703e-4952-d9d3-22e12e43d73d","executionInfo":{"status":"ok","timestamp":1585318518822,"user_tz":0,"elapsed":1549,"user":{"displayName":"Mark Bell","photoUrl":"","userId":"06834569932526992339"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["from google.colab import drive\n","drive.mount('/content/gdrive')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"mX8LgpKkMTp5","colab_type":"code","outputId":"efaefba1-8d78-40e3-b4da-e5be4cab2d0a","executionInfo":{"status":"ok","timestamp":1585318553941,"user_tz":0,"elapsed":1051,"user":{"displayName":"Mark Bell","photoUrl":"","userId":"06834569932526992339"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["data_folder = \"/content/gdrive/My Drive/MLC/Session 3/Data/\""],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['taxonomyids_and_names.txt', 'taxonomy_descriptions.txt']"]},"metadata":{"tags":[]},"execution_count":6}]},{"cell_type":"markdown","metadata":{"id":"acrFqgYmmwmF","colab_type":"text"},"source":["This piece of code imports the libraries that we will be using in the first part of this notebook."]},{"cell_type":"code","metadata":{"id":"WY6Or3c1tB-m","colab_type":"code","colab":{}},"source":["import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import os\n","import sklearn      # The most common Python Machine Learning library - scikit learn\n","from sklearn.model_selection import train_test_split  # Used to create training and test data\n","from sklearn.metrics import confusion_matrix, accuracy_score, f1_score, balanced_accuracy_score\n","import seaborn as sns\n","from sklearn import svm\n","from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n","from sklearn.naive_bayes import GaussianNB, MultinomialNB\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.linear_model import SGDClassifier, LogisticRegression\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n","from sklearn.metrics import precision_score, recall_score, confusion_matrix, classification_report, accuracy_score, f1_score\n","from operator import itemgetter"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6TpTTMJ2nEjN","colab_type":"text"},"source":["Here we will be creating the dataframe (what this library - Pandas - calls a table) from the text file, and load it into a variable called **descriptions**. After loading we will output a count of rows. It should be obvious from the counts that this is a sample of Discovery, not the whole thing (with its 30+ million records). You will notice there are some rows with blanks in the Description column (because the count is different to the other columns). **What should we do with those?** Think back to session 1 when we discussed strategies for missing data."]},{"cell_type":"code","metadata":{"id":"4RTzMIbuZRN7","colab_type":"code","colab":{}},"source":["descriptions = pd.read_csv(data_folder + 'taxonomy_descriptions.txt',\n","                           delimiter=\"|\", header=None, lineterminator='\\n')\n","descriptions.columns = [\"IAID\",\"TAXID\",\"Description\"]\n","descriptions.count()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vOSLxp1WOeQ0","colab_type":"text"},"source":["Let's drop those rows. Without a description there are no features for the ML to work with. In reality we would want to write a description, although that is probably a lifetime's work for the whole catalogue!"]},{"cell_type":"code","metadata":{"id":"T611tdICOVsv","colab_type":"code","colab":{}},"source":["descriptions.dropna(inplace=True)\n","print(\"Rows:\", len(descriptions))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SzsWfXEQJJcy","colab_type":"text"},"source":["### **Understanding our data**\n","\n","We will start by looking at the first few rows. If you want to see more rows you can change the number inside of the brackets after the 'head' function. You can also adjust the 'max_colwidth' setting to see more of the Descriptions if you like."]},{"cell_type":"code","metadata":{"id":"To-mgu2BCcJS","colab_type":"code","colab":{}},"source":["pd.set_option('display.max_colwidth', 130) # increment 5 or 10 at a time if you want to see wider descriptions.\n","print(\"Number of rows:\", len(descriptions))\n","descriptions.head(5)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BkEdnTFIZ_xm","colab_type":"text"},"source":["Next we will load a table of taxonomy category names which will be useful for understanding the various categories."]},{"cell_type":"code","metadata":{"id":"lNkdyBwML5Xg","colab_type":"code","colab":{}},"source":["taxonomy = pd.read_csv(data_folder + 'taxonomyids_and_names.txt',\n","                           delimiter=\"|\", header=None, lineterminator='\\n')\n","taxonomy.columns = [\"TAXID\",\"TaxonomyCategory\"]\n","print(\"Number of rows:\",len(taxonomy))\n","taxonomy.head(10)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LBUjEJJFbkml","colab_type":"text"},"source":["With the list above to hand, you can view sample records for each category. Just change the value of the TAXID variable below with one from the list."]},{"cell_type":"code","metadata":{"id":"0fB4ef3ea6uD","colab_type":"code","colab":{}},"source":["TAXID = 'C10004'  # <- Change this value\n","descriptions[descriptions.TAXID == TAXID].head(5)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fdM_FoUTp1YL","colab_type":"text"},"source":["The previous list was just from a lookup table. We can join this lookup to our Discovery data to get counts by category, with descriptions. The following code produces a summary table showing counts per category. **Are you surprised by the top category?**\n","\n","Change the value of N if you wish to see more rows.\n","\n"]},{"cell_type":"code","metadata":{"id":"SAdG1orTX-FQ","colab_type":"code","colab":{}},"source":["N = 10\n","topN = pd.merge(descriptions, taxonomy, how = 'inner').groupby(['TAXID', 'TaxonomyCategory']).size().reset_index(name='Count').sort_values('Count', ascending=False).head(N)\n","topN"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RkioS0MFcbBS","colab_type":"text"},"source":["It is probably worth looking at some sample rows for 'Food and drink' to check if they look reasonable..."]},{"cell_type":"code","metadata":{"id":"DnDD5Vv4cZ93","colab_type":"code","colab":{}},"source":["TAXID = 'C10039'  # <- Change this value\n","descriptions[descriptions.TAXID == TAXID].head(10)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TWsypJIMkx7L","colab_type":"text"},"source":["Something odd has happened there. We could ask for the top 20, or 50, or 100 rows but there are over 23,000 of them. A better way to summarise the records might be to count words. So let's do that for the food and drink category (you can also change the TAXID value to look at other categories if you want to, as well as varying the MAX_FEATURES variable to see more top words). "]},{"cell_type":"code","metadata":{"id":"sP_FWZAWgwna","colab_type":"code","colab":{}},"source":["TAXID = 'C10039'\n","MAX_FEATURES = 10\n","\n","count_vectorizer = CountVectorizer(max_features = MAX_FEATURES)\n","word_counts = count_vectorizer.fit_transform(descriptions[descriptions.TAXID == TAXID].Description)\n","tfidf_vocab = count_vectorizer.get_feature_names()\n","\n","nz = word_counts.nonzero()\n","ft_names = count_vectorizer.get_feature_names()\n","counts = [x for x in zip(ft_names,np.asarray(word_counts.sum(axis=0))[0])]\n","counts.sort(key=itemgetter(1), reverse=True)\n","counts"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TqBX9EqKl-gk","colab_type":"text"},"source":["This suggests something is wrong with our data. It was sourced from a test system and so there may have been an issue when it was loaded, or somewhere else in the pipeline. For this exercise we will leave it out of our ML process, but hopefully it provides a good example of why digging into the data is so important.\n","\n","**Check the word counts of some of the other categories in the top 10 to reassure yourself that they're ok**\n","\n","**If there are any you don't like the look of, add them to the EXCLUDE list below.** (to add to the EXCLUDE list you will need to separate values with a comma and remember to put them in quotes e.g. ['ABC','DEF','GHI']). We are going to generate the dataset for the remaining tutorials from the list produced below."]},{"cell_type":"code","metadata":{"id":"k-7lLXzznUAW","colab_type":"code","colab":{}},"source":["EXCLUDE = ['C10039']\n","N = 10\n","topN = pd.merge(descriptions[~descriptions['TAXID'].isin(EXCLUDE)], taxonomy, how = 'inner').groupby(['TAXID', 'TaxonomyCategory']).size().reset_index(name='Count').sort_values('Count', ascending=False).head(N)\n","topN"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZI3BUcKDo_HZ","colab_type":"text"},"source":["To end this tutorial, we will write a new dataset using only the top 10 categories. This will be used in the next tutorial. You're welcome to use more categories if you wish, but do think of what a confusion matrix with 135 columns and rows will look like before you go for too many!"]},{"cell_type":"code","metadata":{"id":"U1iK-zUzpaIW","colab_type":"code","colab":{}},"source":["descriptions[descriptions.TAXID.isin(topN.TAXID)].to_csv(data_folder + \"topN_taxonomy.csv\",sep = \"|\")"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5j8xFgt00xaF","colab_type":"text"},"source":["That's the end of this tutorial. We have:\n","\n","\n","\n","*   Loaded the Taxonomy data\n","*   Produced category summaries\n","\n","*   Produced word counts by category\n","*   Identified a large data quality issue\n","\n","*   Created a new data extract for the next tutorial\n","\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"w_TgLRh2PcG7","colab_type":"text"},"source":["Possible additions:\n","\n","Print rules out for a category.\n","Identify key words per class which are unique to that class."]}]}